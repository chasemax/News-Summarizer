{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a collection of articles from each news source\n",
    "# And calculates the top nouns for each article in that news source\n",
    "\n",
    "def top_proper_nouns_per_article(collection, nlp, prop_noun_threshold=0.1):\n",
    "    collection_noun_choices = []\n",
    "\n",
    "    # Loop over all the articles in the news source\n",
    "    for doc in collection:\n",
    "\n",
    "        # We extract all the proper nouns from the article using spacy's library\n",
    "        processed_doc = nlp(doc)\n",
    "        all_prop_nouns = [token.lemma_ for token in processed_doc if token.pos_ == \"PROPN\"]\n",
    "        prop_noun_count = len(all_prop_nouns)\n",
    "        \n",
    "        # Next, we count up how many times each proper noun appeared in the article\n",
    "        noun_counts = dict()\n",
    "\n",
    "        for noun in all_prop_nouns:\n",
    "            if noun in noun_counts:\n",
    "                noun_counts[noun] += 1\n",
    "            else:\n",
    "                noun_counts[noun] = 1\n",
    "\n",
    "        # Finally, we filter out the nouns that appeared less than a minimum threshold\n",
    "        final_noun_choices = [k for k,v in noun_counts.items() if (v / prop_noun_count) > prop_noun_threshold]\n",
    "\n",
    "        collection_noun_choices.append(final_noun_choices)\n",
    "\n",
    "    # We return the list of documents along with the nouns we selected for each document\n",
    "    return list(zip(collection, collection_noun_choices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports and modifications are necessary for the two summarizer functions\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "updated_punctuation = punctuation + \"”\"\n",
    "\n",
    "# This function was mostly copied from https://www.activestate.com/blog/how-to-do-text-summarization-with-python/\n",
    "# It extracts the sentences from a text that contain the most important keywords in that text\n",
    "def extraction_summarize(text, target_length, nlp):\n",
    "    doc = nlp(text)\n",
    "    word_frequencies = {}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in updated_punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "    sentence_tokens = [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "    sentence_tuples = [(sentence, score) for sentence, score in sentence_scores.items()]\n",
    "    sentence_tuples.sort(key = lambda x: x[1], reverse=True)\n",
    "    summary = \"\"\n",
    "\n",
    "    sentence_number = 0\n",
    "\n",
    "    while (len(summary.split()) < target_length and sentence_number < len(sentence_tuples)):\n",
    "        summary += sentence_tuples[sentence_number][0].text\n",
    "        sentence_number += 1\n",
    "        \n",
    "    return summary\n",
    "\n",
    "# This function performs abstraction summarization on a given text, using neural networks to \n",
    "# write a whole new summary for the given text.\n",
    "def abstraction_summarize(original_text):\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "    summary_text = summarizer(original_text, min_length = 100)\n",
    "    return summary_text[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting news source: The Hacker News |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Getting news source: Krebs on Security |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Getting news source: Threatpost |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Getting news source: Naked Security |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Getting news source: Davinci Forensics |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Getting news source: Tech Republic |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Getting news source: Computer World |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "# These imports are necessary for the HTML parser to work\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "\n",
    "# This function returns whether or not a particular tag is visible to the viewer of a web page\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# This function extracts the readable content text from a webpage\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.find_all('p', text=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "    return u\" \".join(t.get_text() for t in visible_texts)\n",
    "\n",
    "# This function defines a progress bar that we can use to show how far along we are in the process\n",
    "# It was taken from https://stackoverflow.com/questions/3173320/text-progress-bar-in-terminal-with-block-characters\n",
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "\n",
    "# This is a list of the RSS feeds we are subscribing to for security news\n",
    "newsSources = {\n",
    "    'The Hacker News': 'https://feeds.feedburner.com/TheHackersNews?format=xml',\n",
    "    #'Graham Cluley': 'https://www.grahamcluley.com/feed/',\n",
    "    'Krebs on Security': 'http://krebsonsecurity.com/feed/',\n",
    "    'Threatpost': 'https://threatpost.com/feed/',\n",
    "    'Naked Security': 'https://nakedsecurity.sophos.com/feed/',\n",
    "    'Davinci Forensics': 'https://davinciforensics.co.za/cybersecurity/feed/',\n",
    "    'Tech Republic': 'https://www.techrepublic.com/rssfeeds/topic/security/?feedType=rssfeeds',\n",
    "    'Computer World': 'https://www.computerworld.com/uk/category/security/index.rss'\n",
    "}\n",
    "\n",
    "########################### MAIN CODE #########################\n",
    "\n",
    "source_articles = dict()\n",
    "\n",
    "for title, source in newsSources.items():\n",
    "\n",
    "    articleTexts = []\n",
    "\n",
    "    # For every news source, we get the RSS feed. \n",
    "    feed = feedparser.parse(source)\n",
    "\n",
    "    l = len(feed['items'])\n",
    "\n",
    "    printProgressBar(0, l, prefix = 'Getting news source: ' + title, suffix = 'Complete', length = 50)\n",
    "\n",
    "    for i, article in enumerate(feed['items']):\n",
    "\n",
    "        #For every article in the feed, we open the web page\n",
    "        html = urllib.request.urlopen(article['link']).read()\n",
    "\n",
    "        #Then we extract the text from the web page and put it in an object\n",
    "        articleTexts.append(text_from_html(html))\n",
    "\n",
    "        printProgressBar(i + 1, l, prefix = 'Getting news source: ' + title, suffix = 'Complete', length = 50)\n",
    "\n",
    "    source_articles[source] = articleTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Proper Nouns |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Topics to summarize are:  ['Windows', 'Security', 'LastPass', 'DevOps', 'Google', 'January', 'March', 'Android', 'malware', 'Ukraine']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the natural language processing library beforehand so we don't have to load it each time we make a call to it\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_sources_proper_noun_counts = dict()\n",
    "\n",
    "l = len(source_articles)\n",
    "i = 0\n",
    "printProgressBar(0, l, prefix = 'Extracting Proper Nouns', suffix = 'Complete', length = 50)\n",
    "\n",
    "for source, articles in source_articles.items():\n",
    "\n",
    "    i += 1\n",
    "    printProgressBar(i, l, prefix = 'Extracting Proper Nouns', suffix = 'Complete', length = 50)\n",
    "    source_proper_noun_counts = top_proper_nouns_per_article(articles, nlp=nlp)\n",
    "\n",
    "    all_sources_proper_noun_counts[source] = source_proper_noun_counts\n",
    "\n",
    "# Calculate the top 10% of proper nouns\n",
    "all_proper_nouns = {}\n",
    "\n",
    "for source, articles in all_sources_proper_noun_counts.items():\n",
    "    for article_text, noun_choices in articles:\n",
    "        for noun in noun_choices:\n",
    "            if noun in all_proper_nouns:\n",
    "                all_proper_nouns[noun] += 1\n",
    "            else:\n",
    "                all_proper_nouns[noun] = 1\n",
    "\n",
    "sorted_proper_nouns = sorted(all_proper_nouns.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_article_nouns = [noun for noun, count in all_proper_nouns.items() if count > 1]\n",
    "top_article_nouns = top_article_nouns[:10]\n",
    "print(\"Topics to summarize are: \", top_article_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing News |█████---------------------------------------------| 10.0% Complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxfi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:953: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:77.)\n",
      "  obj = cast(Storage, torch.UntypedStorage(nbytes))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing News |██████████████████████████████████████████████████| 100.0% Complete\n"
     ]
    }
   ],
   "source": [
    "# Now let's get all the articles that contain each noun\n",
    "top_articles = dict()\n",
    "all_articles = []\n",
    "for articles in all_sources_proper_noun_counts.values():\n",
    "    all_articles.extend(articles)\n",
    "for noun in top_article_nouns:\n",
    "    top_articles[noun] = [article_text for article_text, noun_choices in all_articles if noun in noun_choices]\n",
    "\n",
    "all_summaries = {}\n",
    "\n",
    "l = 10\n",
    "i = 0\n",
    "printProgressBar(0, l, prefix = 'Summarizing News', suffix = 'Complete', length = 50)\n",
    "\n",
    "for topic, articles in top_articles.items():\n",
    "    full_text = \"\"\n",
    "    for article in articles:\n",
    "        full_text += article + \" \"\n",
    "    \n",
    "    i += 1\n",
    "    printProgressBar(i, l, prefix = 'Summarizing News', suffix = 'Complete', length = 50)\n",
    "    all_summaries[topic] = abstraction_summarize(extraction_summarize(full_text, 500, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------\n",
      "\n",
      "Topic: Windows \n",
      "\n",
      " January brings 10 critical updates as well as 67 patches rated as important to the Windows platform . With Windows 10 21H2 now out of mainstream support, we have the following Microsoft applications that will reach end of support or servicing in 2023 . With all of these more difficult testing scenarios, we recommend that you scan your application portfolio for updated application components or system-level dependencies . Given the large number of changes included this month, I have broken down the testing scenarios into high risk and standard risk groups .\n",
      "\n",
      "---------------\n",
      "\n",
      "Topic: Security \n",
      "\n",
      " Application security and API security are two critical components of a comprehensive security strategy . Application security helps protect data and systems from unauthorized access, modification, or data destruction by utilizing techniques around authentication and authorization, encryption, access control, secure coding practices, and more . By implementing proper API security measures, organizations can ensure that their applications remain secure and protected from potential threats . With Noname Security, you can monitor API traffic in real-time to uncover insights into data leakage, data tampering, data policy violations and suspicious behavior .\n",
      "\n",
      "---------------\n",
      "\n",
      "Topic: LastPass \n",
      "\n",
      " LastPass says a threat actor was able to leverage valid credentials stolen from a senior DevOps engineer to access a shared cloud storage environment . The attack happened as a result of the same adversary launching a second attack on its systems . The threat actor pivoted from the first incident, which ended on 2022-08-12, but was actively engaged in a new series of reconnaissance, enumeration, and exfiltration activities . But you don't know, and you can’t easily figure out, what you’re not sure, what the crooks did with it .\n",
      "\n",
      "---------------\n",
      "\n",
      "Topic: DevOps \n",
      "\n",
      " LastPass says one of its DevOps engineers had their personal home computer breached and infected with a keylogger . The employee's passwords are said to have been siphoned by targeting the individual's home computer and leveraging a \"vulnerable third-party media software package\" to achieve remote code execution . LastPass said it upgraded its security posture by rotating critical and high privilege credentials and reissuing certificates obtained by the threat actor . The company said it applied extra S3 hardening measures to put in place logging and alerting mechanisms . The attack happened as a result of the same adversary launching a second attack on its systems .\n",
      "\n",
      "---------------\n",
      "\n",
      "Topic: Google \n",
      "\n",
      " ACLU and eight federal public defenders are asking the Fourth Circuit Court of Appeals to exclude mobile device location data obtained from Google via a so-called geofence warrant that helped law enforcement catch a bank robbery suspect . Mozilla refutes Snapchat, TikTok and Twitter's claims that their apps don't \"share user data with other companies or organizations\" An investigation into data safety labels for Android apps available on the Google Play Store has uncovered \"serious loopholes\" that allow apps to provide misleading or outright false information .\n",
      "\n",
      "---------------\n",
      "\n",
      "Topic: January \n",
      "\n",
      " Dutch police arrest three people in connection with a \"large-scale\" criminal operation involving data theft, extortion, and money laundering . Police say the suspects were blackmailing victims for more-than-grown-up sums of money . They bought decryption keys for 155 victims, but then pulled the rug out of under crooks before the payment went through . The cops figured out a lawfully approved way to disown their payments on the blockchain (and thus to retain their Bitcoins) immediately after getting the keys but before the criminals could claim the cryptocash .\n",
      "\n",
      "---------------\n",
      "\n",
      "Topic: March \n",
      "\n",
      " Dutch police arrest three people in connection with a \"large-scale\" criminal operation involving data theft, extortion, and money laundering . The suspects include two 21-year-old men from Zandvoort and Rotterdam and an 18-year old man without a permanent residence . The police didn’t know who was behind the blackmail demands, but they were able to “cheat the crooks back” by buying decryption keys for 155 victims, but then pulling the rug out of under the crook before the payment went through .\n",
      "\n",
      "---------------\n",
      "\n",
      "Topic: Android \n",
      "\n",
      " Browser-centric password managers still lack many of the more advanced security features the standalone password managers provide . 1Password provides the best all-around Android password management package for most people . Bitwarden is a bit less fully featured and pleasant to use, but it gets the basics right . It includes a generous free offering as well as an open-source model that provides the option to store your encrypted vault on your own self-hosted server . Blur lets you create a variety of special masked email addresses and forward to whatever actual email address you want .\n",
      "\n",
      "---------------\n",
      "\n",
      "Topic: malware \n",
      "\n",
      " The group may be having an interest in industry verticals that are involved in COVID-19-related treatments or vaccines . The start of the infection chain is most likely a phishing message containing a resume-themed lure document that, when launched, grants initial access to the machine . The standout aspects of the campaign is the absence of data exfiltration and custom malware, with the threat actor employing open source tools for intelligence gathering . The goal, it appears, is to not only confuse attribution efforts, but also to make the attacks stealthier .\n",
      "\n",
      "---------------\n",
      "\n",
      "Topic: Ukraine \n",
      "\n",
      " Russia's cyber attacks against Ukraine surged by 250% in 2022 when compared to two years ago, Google's Threat Analysis Group and Mandiant disclosed in a new joint report . Russia’s military intelligence service, the GRU, “launched destructive wiper attacks on hundreds of systems in Ukrainian government, IT, energy, and financial organizations,” Microsoft said . Despite Russia's conventional military setbacks and its failure to substantively advance its agenda through cyber operations, Russia maintains its intent to bring Ukraine under Russian control .\n"
     ]
    }
   ],
   "source": [
    "for topic, summary in all_summaries.items():\n",
    "    print(\"\\n---------------\\n\")\n",
    "    print(\"Topic:\", topic, \"\\n\")\n",
    "    print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "777c797f3d76ce7bff5edf599e7a6aa4adabe4f1940962bf4a5a933b2d8fa9f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
